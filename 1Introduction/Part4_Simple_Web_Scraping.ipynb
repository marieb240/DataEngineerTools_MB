{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requ√™te HTTP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un requ√™te HTTP est une requ√™te bas√©e sur le protocole TCP, elle fait partie de la couche application de la couche OSI. Elle permet d'acc√©der aux donn√©es mise √† disposition sur une adresse IP (ou url r√©solue par un DNS) et un port. \n",
    "\n",
    "Les deux ports les plus utilis√©s dans le web sont le 80 pour les sites en HTTP et le 443 pour les sites en HTTPS. HTTPS est une variable du protocole HTTP bas√© sur le protocole TLS.\n",
    "\n",
    "Il existe de nombreux types de requ√™tes selon la convention `REST`: \n",
    "- GET\n",
    "- POST\n",
    "- PUT \n",
    "- DELETE\n",
    "- UPDATE.\n",
    "\n",
    "Dans notre cas, nous allons utiliser la plupart du temps des GET et potentiellement des POST. \n",
    "- Le GET permet comme son nom l'indique de r√©cup√©rer des informations en fonction de certains param√®tres. \n",
    "- Le POST n√©cessite un envoi de donn√©es pour r√©cup√©rer des donn√©es. Le body du post est, la plupart du temps, envoy√© sous la forme d'un objet JSON.\n",
    "\n",
    "Ces requ√™tes encapsulent un certain nombre de param√®tres qui permettent soient d'identifier une provenance et un utilisateur ou de r√©aliser diff√©rentes actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:47:38.635370Z",
     "start_time": "2024-10-07T19:47:38.599631Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:01.794276Z",
     "start_time": "2024-10-07T19:48:01.690378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.esiee.fr/\"\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe deux m√©thodes pour r√©cup√©rer le contenu de la page :\n",
    "\n",
    "- `response.text` qui permet de retourner le texte sous la forme d'une chaine de charact√®res.\n",
    "- `response.content` qui permet de r√©cup√©rer le contenu de la page sous la forme de bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:06.901620Z",
     "start_time": "2024-10-07T19:48:06.898643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:13.168682Z",
     "start_time": "2024-10-07T19:48:13.166007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour r√©cup√©rer les 1000 premiers charact√®res de la page :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:15.251081Z",
     "start_time": "2024-10-07T19:48:15.248157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"fr-FR\">\\n<head>\\n\\n<meta charset=\"utf-8\">\\n<!-- \\n\\tThis website is powered by TYPO3 - inspiring people to share!\\n\\tTYPO3 is a free open source Content Management Framework initially created by Kasper Skaarhoj and licensed under GNU/GPL.\\n\\tTYPO3 is copyright 1998-2025 of Kasper Skaarhoj. Extensions are copyright of their respective owners.\\n\\tInformation and contribution at https://typo3.org/\\n-->\\n\\n\\n\\n<title>ESIEE Paris, l&#039;√©cole de l&#039;innovation technologique | ESIEE Paris</title>\\n<meta name=\"generator\" content=\"TYPO3 CMS\" />\\n<meta name=\"description\" content=\"Rejoignez ESIEE Paris, grande √©cole d&#039;ing√©nieur dans les domaines des transitions num√©rique, √©nerg√©tique et environnementale. Class√©e dans le groupe A, parmi les meilleures √©coles d&#039;ing√©nieur selon le classement de l&#039;Etudiant. Habilit√©e par la Commission des Titres d&#039;Ing√©nieur (CTI). Membre de la Conf√©rence des Grandes Ecoles (CGE). \" />\\n<meta name=\"viewport\" content=\"width=device-width'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour r√©cup√©rer les headers HTTP de la r√©ponse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:23.287126Z",
     "start_time": "2024-10-07T19:48:23.284018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Tue, 18 Nov 2025 13:27:20 GMT', 'Server': 'Apache', 'Content-Language': 'fr', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'X-UA-Compatible': 'IE=edge', 'X-Content-Type-Options': 'nosniff', 'Content-Length': '16642', 'Content-Type': 'text/html; charset=utf-8', 'X-Varnish': '533630589 535791364', 'Age': '73', 'Via': '1.1 varnish (Varnish/7.1)', 'Accept-Ranges': 'bytes', 'Connection': 'keep-alive'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut modifier les param√®tres de la requ√™te et/ou ses headers. On peut par exemple ajouter un UserAgent (identifiant de l'initiateur de la requ√™te) et un timeout de 10 secondes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:38.532299Z",
     "start_time": "2024-10-07T19:48:38.452433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html lang=\"fr-FR\">\\n<head>\\n\\n<meta charset=\"utf-8\">\\n<!-- \\n\\tThis website is powered by TYPO3 - inspiring people to share!\\n\\tTYPO3 is a free open source Content Management Framework initially created by Kasper Skaarhoj and licensed under GNU/GPL.\\n\\tTYPO3 is copyright 1998-2025 of Kasper Skaarhoj. Extensions are copyright of their respective owners.\\n\\tInformation and contribution at https://typo3.org/\\n-->\\n\\n\\n\\n<title>ESIEE Paris, l&#039;\\xc3\\xa9cole de l&#039;innovation technologique | ESIEE Paris</title>\\n<meta name=\"generator\" content=\"TYPO3 CMS\" />\\n<meta name=\"description\" content=\"Rejoignez ESIEE Paris, grande \\xc3\\xa9cole d&#039;ing\\xc3\\xa9nieur dans les domaines des transitions num\\xc3\\xa9rique, \\xc3\\xa9nerg\\xc3\\xa9tique et environnementale. Class\\xc3\\xa9e dans le groupe A, parmi les meilleures \\xc3\\xa9coles d&#039;ing\\xc3\\xa9nieur selon le classement de l&#039;Etudiant. Habilit\\xc3\\xa9e par la Commission des Titres d&#039;Ing\\xc3\\xa9nieur (CTI). Membre de la Conf\\xc3\\xa9rence des Grandes Ecoles (CGE). \" />\\n<meta name=\"viewport\" content=\"width='"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers, timeout = 10)\n",
    "response.content[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1\n",
    "\n",
    "- Cr√©er une classe Python permettant de faire des requ√™tes HTTP.\n",
    "- Cette classe doit utiliser toujours le m√™me UserAgent.\n",
    "- Le TimeOut sera sp√©cifi√© √† chaque appelle avec une valeur par d√©faut.\n",
    "- Un m√©canisme de retry sera mis en place de fa√ßon recursive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HttpClient:\n",
    "    def __init__(self, user_agent: str = \"MonSuperScraper/1.0\"):\n",
    "        \"\"\"\n",
    "        Constructeur de la classe, on d√©finit ici le user agent qui sera utilis√© pour toutes les requ√™tes\n",
    "        \"\"\"\n",
    "        self.user_agent = user_agent\n",
    "        self.headers = {\"user-agent\": self.user_agent}\n",
    "\n",
    "    def get(self, url: str, timeout: float = 5.0, retries: int = 3):\n",
    "        \"\"\"\n",
    "        Effectue une requ√™te HTTP GET avec :\n",
    "        - un timeout sp√©cifiable\n",
    "        - un m√©canisme de retry r√©cursif\n",
    "\n",
    "        url: URL √† appeler\n",
    "        timeout: dur√©e max d'attente en secondes\n",
    "        retries: nombre de tentatives restantes\n",
    "        return: l'objet Response si succ√®s, sinon None\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # On tente la requ√™te\n",
    "            response = requests.get(url, headers=self.headers, timeout=timeout)\n",
    "\n",
    "            # Si le code HTTP indique un succ√®s (200, 201, etc.)\n",
    "            response.raise_for_status()  # l√®ve une erreur pour les codes 4xx/5xx\n",
    "\n",
    "            return response\n",
    "\n",
    "        except (requests.exceptions.RequestException) as e:\n",
    "            # En cas d'erreur r√©seau, timeout, mauvais code HTTP,...\n",
    "            print(f\"Erreur lors de la requ√™te vers {url} : {e}\")\n",
    "\n",
    "            if retries > 0:\n",
    "                print(f\"Nouvelle tentative... (retries restants : {retries})\")\n",
    "                # Appel r√©cursif avec un retry en moins\n",
    "                return self.get(url, timeout=timeout, retries=retries - 1)\n",
    "            else:\n",
    "                print(\"Plus de retries disponibles, abandon.\")\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercice 2\n",
    "\n",
    "- Faire une fonction permettant de supprimer tous les espaces supperflus d'une string\n",
    "- Faire une fonction qui prend une string html et renvois une string intelligible (enlever les caract√®res sp√©ciaux,\n",
    "- R√©cup√©rer le domaine en fonction d'un url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour retirer les esapces pr√©sent dans un string\n",
    "def clean_spaces(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Supprime les espaces superflus :\n",
    "    - espaces en d√©but/fin\n",
    "    - plusieurs espaces cons√©cutifs -> un seul\n",
    "    \"\"\"\n",
    "    # fonction .split() coupe sur n'importe quel espace et enl√®ve les doublons\n",
    "    parts = text.split()\n",
    "    # on rejoint avec un seul espace\n",
    "    return \" \".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def html_to_text_simple(html_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforme une string HTML en texte lisible\n",
    "    \"\"\"\n",
    "    # Enlever les balises <...>\n",
    "    #    <[^>]+> veut dire : un '<', puis n'importe quels caract√®res sauf '>', puis un '>' \n",
    "    # on remplace par un espace pour √©viter de coller des mots ensemble\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", html_string)\n",
    "\n",
    "    # D√©coder les entit√©s HTML (&amp; -> &, &eacute; -> √©, etc.)\n",
    "    # on utilise la biblioth√®que standard html\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Nettoyer les espaces superflus avec la fonction d√©finie plus haut\n",
    "    text = clean_spaces(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    \"\"\"\n",
    "    R√©cup√®re le domaine √† partir d'une URL compl√®te.\n",
    "    \"\"\"\n",
    "    # on utilise la fonction urlparse pour analyser l'URL\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc  # ex: 'www.google.com'\n",
    "\n",
    "    # Optionnel : enlever 'www.' au d√©but\n",
    "    if domain.startswith(\"www.\"):\n",
    "        domain = domain[4:]\n",
    "    \n",
    "    return domain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation du HTML  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, il faut r√©cup√©rer le code HTML d'un site web √† partir d'une requ√™te. Lorsque vous avez r√©cup√©r√© le texte d'un site il faut le parser. Pour cela, on utilise BeautifulSoup qui permet de transformer la structure HTML en objet Python. Cela permet de r√©cup√©rer efficacement les donn√©es qui nous int√©resse.\n",
    "\n",
    "Pour les webmasters, le blocage le plus souvent mis en place et un blocage sur le User-Agent. Le User-Agent est un param√®tre int√©gr√© dans la requ√™te HTTP r√©alis√© par le Navigateur pour envoyer au front des informations basiques :\n",
    "\n",
    "- la version du Navigateur,\n",
    "- la version de l'OS\n",
    "- Le type de gestionnaire graphique (Gecko)\n",
    "- le type de device utilis√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de User Agent :  \n",
    "\n",
    "`Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commen√ßons √† utiliser `BeautifulSoup`, il est normalement d√©j√† install√©, le cas √©ch√©ant executez les lignes suivantes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:10.823860Z",
     "start_time": "2024-10-07T19:50:10.764935Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour transformer une requ√™te (requests) en objet BeautifulSoup :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:13.979141Z",
     "start_time": "2024-10-07T19:50:13.888552Z"
    }
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour trouver tous les liens d'une page, on r√©cup√®re la balise `a` qui permet de g√©rer les liens en HTML :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:19.065788Z",
     "start_time": "2024-10-07T19:50:19.063429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/#content\">Aller au contenu</a>,\n",
       " <a href=\"/#menu\">Aller au menu</a>,\n",
       " <a href=\"/plan-du-site/\">Plan du site</a>,\n",
       " <a href=\"/actualites/journees-portes-ouvertes-2025-2026\" target=\"_blank\" title=\"Ouvre une nouvelle fen√™tre\">Journ√©e portes ouvertes le 6 d√©cembre de 13h √† 18h. Inscrivez-vous d√®s maintenant !</a>,\n",
       " <a href=\"/\"><img alt=\"ESIEE PARIS\" class=\"a42-ac-replace-img\" src=\"/typo3conf/ext/esiee_sitepackage/Resources/Public/imgs/svg/logo-esiee.svg\"/></a>,\n",
       " <a href=\"/brochures-1\">Brochures</a>,\n",
       " <a href=\"/informations/etudiantes-et-etudiants\">Espace √©l√®ves</a>,\n",
       " <a href=\"/\" hreflang=\"fr-FR\" title=\"Fran√ßais\">\n",
       " <span>Fr</span>\n",
       " </a>,\n",
       " <a href=\"/en/\" hreflang=\"en-US\" title=\"English\">\n",
       " <span>En</span>\n",
       " </a>,\n",
       " <a href=\"/candidater-1\">Candidater</a>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\")[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi pr√©ciser la classe HTML qu'on veut r√©cup√©rer :\n",
    "\n",
    "```python\n",
    "soup.find_all(class_=\"<CLASS_NAME>\")[0:10]\n",
    "```\n",
    "\n",
    "Ici par exemple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:34.406680Z",
     "start_time": "2024-10-07T19:50:34.400091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<button aria-controls=\"searchbox-header-form\" aria-expanded=\"false\" class=\"toggler\">\n",
       " <i class=\"fa-solid fa-magnifying-glass\"></i>\n",
       " <i class=\"fa-solid fa-xmark\"></i>\n",
       " <span class=\"sr-only\">\n",
       " <span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> la recherche\n",
       " \t\t</span>\n",
       " </button>,\n",
       " <button aria-controls=\"submenu-40\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu¬†: </span>L'√©cole</button>,\n",
       " <button aria-controls=\"submenu-563\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu¬†: </span>Gouvernance et conseils</button>,\n",
       " <button aria-controls=\"submenu-65\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu¬†: </span>D√©partements d'enseignements et de recherche</button>,\n",
       " <button aria-controls=\"submenu-67\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu¬†: </span>Salles blanches</button>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_=\"toggler\")[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour r√©cup√©rer le text sans les balises HTML :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:43.103789Z",
     "start_time": "2024-10-07T19:50:43.100752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\nESIEE Paris, l'√©cole de l'innovation technologique | ESIEE Paris\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAller au contenu\\nAller au menu\\nPlan du site\\n\\n\\n\\n\\n\\n\\n\\nJourn√©e portes ouvertes le 6 d√©cembre de 13h √† 18h. Inscrivez-vous d√®s maintenant !\\n\\n\\n\\n\\n\\nMasquer l'alerte\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBrochuresEspace √©l√®ves\\n\\n\\n\\nFr\\n\\n\\n\\n\\nEn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAfficherMasquer la recherche\\r\\n\\t\\t\\n\\n\\n\\nSaisissez votre recherche\\xa0:\\n\\nLancer la recherche\\n\\n\\n\\nCandidater\\n\\nAfficherMasquer le menu\\n\\n\\n\\n\\n\\nRetour au menu principalAfficherMasquer le sous menu\\xa0: L'√©colePourquoi choisir ESIEE Paris ?AfficherMasquer le sous menu\\xa0: Gouvernance et conseilsGouvernance et conseilsConseil scientifiqueAfficherMasquer le sous menu\\xa0: D√©partements d'enseignements et de rechercheInformatique et t√©l√©communicationsIng√©nierie des syst√®mes cyberphysiquesIng√©nierie industrielleSant√©, √©nergie et environnement durableManagement, sciences humaines et languesCorps professoralD√©marche Qualit√© et DD&RSAfficherMasquer le sous menu\\xa0: Salles blanchesSalles blanches√âquipements et proc√©\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "### Exercice 3\n",
    "\n",
    "Am√©liorer la classe d√©velopp√© pr√©c√©demment.\n",
    "\n",
    "- Ajouter une m√©thode pour r√©cup√©rer l'objet soup d'un url\n",
    "- R√©cup√©rer une liste de User Agent et effectuer une rotation al√©atoire sur celui √† utiliser\n",
    "- Utiliser cette classe pour parser une page HTML et r√©cup√©rer : le titre, tous les H1 (si ils existent), les liens vers les images, les liens sortants vers d'autres sites, et le texte principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "class HttpClient:\n",
    "    def __init__(self, user_agents=None):\n",
    "        \"\"\"\n",
    "        user_agents : liste de cha√Ænes User-Agent.\n",
    "        Si None, on utilise une petite liste par d√©faut.\n",
    "        \"\"\"\n",
    "        if user_agents is None:\n",
    "            self.user_agents = [\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "                \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "                \"MonSuperScraper/1.0\"\n",
    "            ]\n",
    "        else:\n",
    "            self.user_agents = user_agents\n",
    "\n",
    "    def _get_random_headers(self):\n",
    "        \"\"\"\n",
    "        Choisit un User-Agent au hasard et construit les headers.\n",
    "        \"\"\"\n",
    "        ua = random.choice(self.user_agents)\n",
    "        return {\"User-Agent\": ua}\n",
    "\n",
    "    def get(self, url: str, timeout: float = 5.0, retries: int = 3):\n",
    "        \"\"\"\n",
    "        Effectue une requ√™te HTTP GET avec :\n",
    "        - rotation al√©atoire de User-Agent\n",
    "        - timeout\n",
    "        - retries r√©cursifs\n",
    "        \"\"\"\n",
    "        try:\n",
    "            headers = self._get_random_headers()\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            response.raise_for_status()  # l√®ve une exception si code HTTP 4xx/5xx\n",
    "            return response\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur lors de la requ√™te vers {url} : {e}\")\n",
    "\n",
    "            if retries > 0:\n",
    "                print(f\"Nouvelle tentative... (retries restants : {retries})\")\n",
    "                return self.get(url, timeout=timeout, retries=retries - 1)\n",
    "            else:\n",
    "                print(\"Plus de retries disponibles, abandon.\")\n",
    "                return None\n",
    "    \n",
    "    def get_soup(self, url: str, timeout: float = 5.0, retries: int = 3):\n",
    "        \"\"\"\n",
    "        R√©cup√®re un objet BeautifulSoup √† partir d'une URL.\n",
    "        \"\"\"\n",
    "        response = self.get(url, timeout=timeout, retries=retries)\n",
    "        if response is None:\n",
    "            return None\n",
    "\n",
    "        # Cr√©e l'objet soup √† partir du HTML\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        return soup\n",
    "    \n",
    "    def parse_page(self, url: str, timeout: float = 5.0, retries: int = 3):\n",
    "        \"\"\"\n",
    "        Parse une page HTML et renvoie :\n",
    "        - title : titre de la page\n",
    "        - h1_list : liste de tous les H1\n",
    "        - image_links : liste des URLs d'images\n",
    "        - external_links : liste des liens sortants vers d'autres domaines\n",
    "        - main_text : texte principal (contenu des <p>)\n",
    "        \"\"\"\n",
    "        soup = self.get_soup(url, timeout=timeout, retries=retries)\n",
    "        if soup is None:\n",
    "            return None\n",
    "\n",
    "        # ----- Titre -----\n",
    "        title_tag = soup.find(\"title\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "        # ----- H1 -----\n",
    "        h1_list = [h.get_text(strip=True) for h in soup.find_all(\"h1\")]\n",
    "\n",
    "        # ----- Images -----\n",
    "        image_links = []\n",
    "        for img in soup.find_all(\"img\"):\n",
    "            src = img.get(\"src\")\n",
    "            if src:\n",
    "                full_url = urljoin(url, src)\n",
    "                image_links.append(full_url)\n",
    "\n",
    "        # ----- Liens sortants -----\n",
    "        parsed_base = urlparse(url)\n",
    "        base_domain = parsed_base.netloc.replace(\"www.\", \"\")\n",
    "\n",
    "        external_links_set = set()\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            full_url = urljoin(url, href)\n",
    "            parsed_link = urlparse(full_url)\n",
    "\n",
    "            if parsed_link.scheme not in (\"http\", \"https\"):\n",
    "                continue\n",
    "\n",
    "            link_domain = parsed_link.netloc.replace(\"www.\", \"\")\n",
    "            if link_domain and link_domain != base_domain:\n",
    "                external_links_set.add(full_url)\n",
    "\n",
    "        external_links = list(external_links_set)\n",
    "\n",
    "        # ----- Texte principal -----\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "        main_text = \" \".join(paragraphs)\n",
    "        main_text = clean_spaces(main_text)\n",
    "\n",
    "        # On renvoie tout dans un dictionnaire\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"h1_list\": h1_list,\n",
    "            \"image_links\": image_links,\n",
    "            \"external_links\": external_links,\n",
    "            \"main_text\": main_text\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation des appels d'API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Losque le front du site r√©cup√®re des donn√©es sur une API g√©r√©e par le back, un appel d'API est r√©alis√©. Cet appel est recens√© dans les appels r√©seaux. Il est alors possible de re-jouer cet appel pour r√©cup√©rer √† nouveau les donn√©es. Il est tr√®s facile de r√©cup√©rer ces appels dans l'onglet Network de la console d√©veloppeur de Chrome ou FireFox. La console vous permet de copier le code CURL de la requ√™te et vous pouvez ensuite la transformer en code Python depuis le site https://curl.trillworks.com/.\n",
    "\n",
    "Souvent les APIs sont bloqu√©es avec certains param√®tres. L'API v√©rifie que dans les headers de la requ√™te HTTP ces param√®tres sont pr√©sents :\n",
    "* un token g√©n√©r√© √† la vol√©e avec des protocoles OAuth2 (ou moins d√©velopp√©s).\n",
    "* un referer provenant du site web (la source de la requ√™te), tr√®s facile √† falsifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice \n",
    "### Exercice 4\n",
    "\n",
    "- Utiliser les informations d√©velopp√©es plus haut pour r√©cup√©rer les premiers r√©sultats d'une recherche d'une requ√™te\n",
    "sur Google. \n",
    "\n",
    "Tips : \n",
    "\n",
    "- Ouvrir les outils de d√©veloppements de Chrome ou Firefox\n",
    "- Onglet Network\n",
    "- Fouiller dans les requ√™tes pour voir √† quoi ressemble un appel API Google\n",
    "- Utilisez beautiful soup pour convertir le contenu de la request en objet et acc√©der aux balises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(self, url: str, timeout: float = 5.0, retries: int = 3, parser: str = \"html.parser\"):\n",
    "        \"\"\"\n",
    "        R√©cup√®re un objet BeautifulSoup √† partir d'une URL.\n",
    "        parser : \"html.parser\" (par d√©faut) ou \"xml\" pour les flux RSS.\n",
    "        \"\"\"\n",
    "        response = self.get(url, timeout=timeout, retries=retries)\n",
    "        if response is None:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, parser)\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote_plus\n",
    "\n",
    "def build_google_search_url(query: str, lang: str = \"fr\") -> str:\n",
    "    \"\"\"\n",
    "    Construit une URL de recherche Google √† partir d'une requ√™te.\n",
    "    \"\"\"\n",
    "    q = quote_plus(query)  # remplace espaces par +, etc.\n",
    "    return f\"https://www.google.com/search?q={q}&hl={lang}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(client: HttpClient, query: str, max_results: int = 5):\n",
    "    \"\"\"\n",
    "    Utilise la classe HttpClient pour r√©cup√©rer les premiers r√©sultats Google d'une requ√™te.\n",
    "    Retourne une liste de dictionnaires\n",
    "    \"\"\"\n",
    "    # Construire l'URL de recherche avec la fonction d√©finie plus haut\n",
    "    search_url = build_google_search_url(query)\n",
    "\n",
    "    #R√©cup√©rer le soup avec notre client\n",
    "    soup = client.get_soup(search_url)\n",
    "    if soup is None:\n",
    "        print(\"Impossible de r√©cup√©rer la page de r√©sultats Google\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Trouver les blocs de r√©sultats : div.g \n",
    "    for result_block in soup.select(\"div.g\"):\n",
    "        # Titre\n",
    "        title_tag = result_block.find(\"h3\")\n",
    "        if not title_tag:\n",
    "            continue  # si pas de titre, on saute ce bloc\n",
    "\n",
    "        title = title_tag.get_text(strip=True)\n",
    "\n",
    "        # Lien\n",
    "        link_tag = result_block.find(\"a\", href=True)\n",
    "        if not link_tag:\n",
    "            continue\n",
    "\n",
    "        url = link_tag[\"href\"]\n",
    "\n",
    "        # Snippet \n",
    "        snippet_tag = result_block.find(\"div\", class_=\"VwiC3b\")\n",
    "        if snippet_tag is None:\n",
    "            snippet_tag = result_block.find(\"span\", class_=\"aCOpRe\")\n",
    "\n",
    "        snippet = \"\"\n",
    "        if snippet_tag is not None:\n",
    "            snippet = snippet_tag.get_text(\" \", strip=True)\n",
    "\n",
    "        results.append({\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"snippet\": snippet\n",
    "        })\n",
    "\n",
    "        if len(results) >= max_results:\n",
    "            break\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice Final  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice Final\n",
    "Utilisez tout ce que vous avez appris pour r√©cup√©rer des articles de News avec une cat√©gorie. Il est souvent int√©ressant de partir des flux RSS pour commencer :\n",
    "\n",
    "Les donn√©es doivent comprendre :\n",
    "- Le texte important propre\n",
    "- L'url\n",
    "- Le domaine\n",
    "- la cat√©gorie\n",
    "- Le titre de l'article\n",
    "- Le titre de la page\n",
    "- (Facultatif) : les images\n",
    "\n",
    "Tips : \n",
    "\n",
    "- Taper le nom de votre m√©dia favoris + RSS (par exemple : https://www.lemonde.fr/rss/)\n",
    "- Aller dans le DOM de la page \n",
    "- Trouver les cat√©gories et les liens vers les articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HttpClient:\n",
    "    def __init__(self, user_agents=None):\n",
    "        if user_agents is None:\n",
    "            self.user_agents = [\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "                \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "                \"MonSuperScraper/1.0\"\n",
    "            ]\n",
    "        else:\n",
    "            self.user_agents = user_agents\n",
    "\n",
    "    def _get_random_headers(self):\n",
    "        ua = random.choice(self.user_agents)\n",
    "        return {\"User-Agent\": ua}\n",
    "\n",
    "    def get(self, url: str, timeout: float = 5.0, retries: int = 3):\n",
    "        try:\n",
    "            headers = self._get_random_headers()\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur lors de la requ√™te vers {url} : {e}\")\n",
    "            if retries > 0:\n",
    "                print(f\"Nouvelle tentative... (retries restants : {retries})\")\n",
    "                return self.get(url, timeout=timeout, retries=retries - 1)\n",
    "            else:\n",
    "                print(\"Abandon.\")\n",
    "                return None\n",
    "\n",
    "    def get_soup(self, url: str, timeout: float = 5.0, retries: int = 3, parser: str = \"html.parser\"):\n",
    "        response = self.get(url, timeout=timeout, retries=retries)\n",
    "        if response is None:\n",
    "            return None\n",
    "        return BeautifulSoup(response.text, parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_rss_items(client: HttpClient, rss_url: str, max_items: int = 5):\n",
    "    \"\"\"\n",
    "    R√©cup√®re les items d'un flux RSS (titre, lien, cat√©gorie √©ventuelle).\n",
    "    Utilise HttpClient.get_soup avec parser=\"xml\".\n",
    "    \"\"\"\n",
    "    soup = client.get_soup(rss_url, parser=\"xml\")\n",
    "    if soup is None:\n",
    "        print(\"Impossible de r√©cup√©rer le flux RSS.\")\n",
    "        return []\n",
    "\n",
    "    items = []\n",
    "    for item in soup.find_all(\"item\")[:max_items]:\n",
    "        title_tag = item.find(\"title\")\n",
    "        link_tag = item.find(\"link\")\n",
    "        category_tag = item.find(\"category\")\n",
    "\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "        link = link_tag.get_text(strip=True) if link_tag else \"\"\n",
    "        category = category_tag.get_text(strip=True) if category_tag else \"\"\n",
    "\n",
    "        items.append({\n",
    "            \"rss_title\": title,\n",
    "            \"url\": link,\n",
    "            \"category\": category\n",
    "        })\n",
    "\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_page(client: HttpClient, article_info: dict):\n",
    "    \"\"\"\n",
    "    Prend un dict contenant au moins:\n",
    "    - 'url'\n",
    "    - 'category' (depuis le RSS, peut √™tre vide)\n",
    "    - 'rss_title'\n",
    "\n",
    "    Retourne un dict avec :\n",
    "    - text : texte important propre\n",
    "    - url\n",
    "    - domain\n",
    "    - category\n",
    "    - article_title\n",
    "    - page_title\n",
    "    - images\n",
    "    \"\"\"\n",
    "    url = article_info[\"url\"]\n",
    "    category = article_info.get(\"category\", \"\")\n",
    "    rss_title = article_info.get(\"rss_title\", \"\")\n",
    "\n",
    "    # Si pas de cat√©gorie dans le RSS, on met une valeur par d√©faut\n",
    "    if not category:\n",
    "        category = \"Ouest-France\"\n",
    "\n",
    "    soup = client.get_soup(url)\n",
    "    if soup is None:\n",
    "        return None\n",
    "\n",
    "    # Titre de la page (<title>)\n",
    "    page_title_tag = soup.find(\"title\")\n",
    "    page_title = page_title_tag.get_text(strip=True) if page_title_tag else \"\"\n",
    "\n",
    "    # Titre de l'article (souvent <h1>)\n",
    "    h1_tag = soup.find(\"h1\")\n",
    "    article_title = h1_tag.get_text(strip=True) if h1_tag else rss_title\n",
    "\n",
    "    # Texte principal : on essaie d'abord <article>, sinon tous les <p>\n",
    "    article_tag = soup.find(\"article\")\n",
    "    if article_tag:\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in article_tag.find_all(\"p\")]\n",
    "    else:\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "\n",
    "    main_text = clean_spaces(\" \".join(paragraphs))\n",
    "\n",
    "    # Images : src des <img>\n",
    "    image_links = []\n",
    "    img_tags = article_tag.find_all(\"img\") if article_tag else soup.find_all(\"img\")\n",
    "    for img in img_tags:\n",
    "        src = img.get(\"src\")\n",
    "        if src:\n",
    "            full_url = urljoin(url, src)\n",
    "            image_links.append(full_url)\n",
    "\n",
    "    # Domaine (ouest-france.fr)\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc\n",
    "    if domain.startswith(\"www.\"):\n",
    "        domain = domain[4:]\n",
    "\n",
    "    return {\n",
    "        \"text\": main_text,\n",
    "        \"url\": url,\n",
    "        \"domain\": domain,\n",
    "        \"category\": category,\n",
    "        \"article_title\": article_title,\n",
    "        \"page_title\": page_title,\n",
    "        \"images\": image_links\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news_from_rss(client: HttpClient, rss_url: str, max_items: int = 5):\n",
    "    \"\"\"\n",
    "    1) R√©cup√®re quelques items depuis le flux RSS\n",
    "    2) Pour chaque item, va sur la page de l'article et la parse\n",
    "    3) Retourne une liste de dicts complets\n",
    "    \"\"\"\n",
    "    rss_items = fetch_rss_items(client, rss_url, max_items=max_items)\n",
    "\n",
    "    articles = []\n",
    "    for item in rss_items:\n",
    "        parsed = parse_article_page(client, item)\n",
    "        if parsed is not None:\n",
    "            articles.append(parsed)\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ARTICLE 1\n",
      "Titre de la page   : Apr√®s Shein, la plateforme eBay vis√©e par une enqu√™te pour vente de produits ill√©gaux\n",
      "Titre de l'article : Apr√®s Shein, la plateforme eBay vis√©e par une enqu√™te pour vente de produits ill√©gaux\n",
      "URL                : https://www.ouest-france.fr/economie/commerce/e-commerce/apres-shein-la-plateforme-ebay-visee-par-une-enquete-pour-vente-de-produits-illegaux-e6c4e986-c9ff-11f0-a0e6-83b9718ad3c0?utm_source=fluxpublicfrance&utm_medium=fluxrss&utm_campaign=banquedecontenu\n",
      "Domaine            : ouest-france.fr\n",
      "Cat√©gorie          : Ouest-France\n",
      "Nb images trouv√©es : 2\n",
      "\n",
      "Texte (d√©but) :\n",
      "Mardi 25 novembre, le parquet de Paris a indiqu√© que la plateforme am√©ricaine eBay √©tait vis√©e par une enqu√™te pour vente de produits ill√©gaux. Quatre autres plateformes, Shein, Temu, Wish et AliExpress sont d√©j√† poursuivies en France. Ouest-France Newsletter Mon Budget Chaque semaine, infos pratiques et conseils utiles pour vos d√©penses du quotidien Merci de saisir votre adresse e-mail Votre e-mail, avec votre consentement, est utilis√© par Ouest-France pour recevoir notre newsletter. En savoir  ...\n",
      "\n",
      "================================================================================\n",
      "ARTICLE 2\n",
      "Titre de la page   : 1¬†000¬†ans de tr√©sors manuscrits expos√©s √† la Cit√© internationale de la langue fran√ßaise\n",
      "Titre de l'article : 1¬†000¬†ans de tr√©sors manuscrits expos√©s √† la Cit√© internationale de la langue fran√ßaise\n",
      "URL                : https://www.ouest-france.fr/culture/1-000-ans-de-tresors-manuscrits-exposes-a-la-cite-internationale-de-la-langue-francaise-0d4f6f94-c9f2-11f0-a0e6-83b9718ad3c0?utm_source=fluxpublicfrance&utm_medium=fluxrss&utm_campaign=banquedecontenu\n",
      "Domaine            : ouest-france.fr\n",
      "Cat√©gorie          : Ouest-France\n",
      "Nb images trouv√©es : 3\n",
      "\n",
      "Texte (d√©but) :\n",
      "Du 5 novembre 2025 au 1er mars 2026, la Cit√© internationale de la langue fran√ßaise de Villers-C√¥terets (Aisne) pr√©sente ¬´ Tr√©sors et secrets d‚Äô√©criture ¬ª, une exposition exceptionnelle rassemblant plus de 100 manuscrits issus des collections de la Biblioth√®que nationale de France. Compilation in√©dite d‚Äôarchives datant du Moyen √Çge √† nos jours, le parcours constitue une invitation √† un voyage extraordinaire au travers de l‚Äôhistoire de l‚Äô√©criture francophone. Ouest-France Communication Newsletter  ...\n",
      "\n",
      "================================================================================\n",
      "ARTICLE 3\n",
      "Titre de la page   : Auchan veut transformer 300¬†supermarch√©s en enseignes Netto et Intermarch√©\n",
      "Titre de l'article : Auchan veut transformer 300¬†supermarch√©s en enseignes Netto et Intermarch√©\n",
      "URL                : https://www.ouest-france.fr/economie/entreprises/auchan/auchan-veut-transformer-ses-300-supermarches-en-enseignes-netto-et-intermarche-ce62c572-c9f3-11f0-a0e6-83b9718ad3c0?utm_source=fluxpublicfrance&utm_medium=fluxrss&utm_campaign=banquedecontenu\n",
      "Domaine            : ouest-france.fr\n",
      "Cat√©gorie          : Ouest-France\n",
      "Nb images trouv√©es : 2\n",
      "\n",
      "Texte (d√©but) :\n",
      "Le distributeur Auchan a annonc√© mardi 25 novembre qu‚Äôil voulait exploiter l‚Äôensemble de ses supermarch√©s en franchise sous banni√®re Intermarch√© et Netto. Le groupe appartient √† la famille Mulliez. Ouest-France Newsletter Entreprises Merci de saisir votre adresse e-mail Votre e-mail, avec votre consentement, est utilis√© par Ouest-France pour recevoir notre newsletter. En savoir plus . En difficult√© depuis plusieurs ann√©es, le distributeur Auchan a annonc√© mardi 25 novembre 2025 vouloir exploiter ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = HttpClient()\n",
    "\n",
    "# üëâ Flux RSS Ouest-France (France enti√®re)\n",
    "rss_url = \"https://www.ouest-france.fr/rss.xml?insee=FRA\"\n",
    "\n",
    "articles = fetch_news_from_rss(client, rss_url, max_items=3)\n",
    "\n",
    "for i, art in enumerate(articles, start=1):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ARTICLE {i}\")\n",
    "    print(\"Titre de la page   :\", art[\"page_title\"])\n",
    "    print(\"Titre de l'article :\", art[\"article_title\"])\n",
    "    print(\"URL                :\", art[\"url\"])\n",
    "    print(\"Domaine            :\", art[\"domain\"])\n",
    "    print(\"Cat√©gorie          :\", art[\"category\"])\n",
    "    print(\"Nb images trouv√©es :\", len(art[\"images\"]))\n",
    "    print(\"\\nTexte (d√©but) :\")\n",
    "    print(art[\"text\"][:500], \"...\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataengineertools (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
